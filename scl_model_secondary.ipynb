{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407368a-2d64-475f-b906-527d05c88a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Super_Contrastive_Loss import SupConLoss\n",
    "from utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29cfe03-c329-4e9b-b3c7-dfe82fa57f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cls_emb.pkl', 'rb') as f:\n",
    "    cls = pickle.load(f)\n",
    "with open('feature_vectors.pkl', 'rb')as f:\n",
    "    feature_vectors= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1194a9a-fa95-4076-9047-94d14d2e4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a0b872-1ef6-4c21-98d9-38541d968211",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv('final_data.csv')\n",
    "map_dict = {'llama3.1-70b':0, 'mistral':1, 'gpt-4o-2024-05-13':2}\n",
    "response_df['model_nums'] = response_df['model'].map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928db930-1efe-410a-a3b4-691ced778826",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [torch.cat((cls[i].float(), torch.from_numpy(feature_vectors[i]).unsqueeze(0).float()), dim=1) for i in range(len(cls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb05e48-b291-4de9-a164-b3360f7ed5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_split(response_df, embeddings, temperature):\n",
    "    temp_idx = response_df[response_df['temperature'] == temperature].index\n",
    "    temp_embs = [embeddings[idx] for idx in temp_idx]\n",
    "    temp_targs = [response_df['model_nums'][idx] for idx in temp_idx]\n",
    "    \n",
    "    return train_test_split(temp_embs, temp_targs, test_size=0.1, random_state=42)\n",
    "    \n",
    "temp_0_train, temp_0_test, temp_0_targs_train, temp_0_targs_test = extract_and_split(response_df, embeddings, 0)\n",
    "temp_7_train, temp_7_test, temp_7_targs_train, temp_7_targs_test = extract_and_split(response_df, embeddings, 0.7)\n",
    "temp_14_train, temp_14_test, temp_14_targs_train, temp_14_targs_test = extract_and_split(response_df, embeddings, 1.4)\n",
    "temp_all_train, temp_all_test, temp_all_targs_train, temp_all_targs_test = train_test_split(embeddings, response_df['model_nums'], \n",
    "                                                                                            test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63705575-6ec2-4d28-b3e4-1f45c0226f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(embed_size, hidden_size)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.2\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch,  dim = text.size()\n",
    "        feat = self.fc(torch.tanh(self.dropout(text.view(batch, dim))))\n",
    "        feat = F.normalize(feat, dim=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd52bf4-0134-4e2d-9d0f-5b2db6c63d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, hidden_size, projection_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size, projection_size)\n",
    "        self.ln = nn.LayerNorm(projection_size)\n",
    "        self.bn = nn.BatchNorm1d(projection_size)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.01\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch,  dim = text.size()\n",
    "        return self.ln(self.fc(torch.tanh(text.view(batch, dim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc98701-4433-49ab-9760-763b214973d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_class, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.02\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        return self.fc(torch.tanh(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d8db8b-6f5d-43b6-8637-7e0da9755196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, cls_embs, targs):\n",
    "        self.cls_embs = cls_embs\n",
    "        self.targs = targs \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cls_embs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cls_embs[idx], self.targs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c91b276-e20c-438c-bf94-3c4dbdd3c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "dataset_0 = WordEmbeddingDataset(temp_0_train, temp_0_targs_train)\n",
    "dataset_0_test = WordEmbeddingDataset(temp_0_test, temp_0_targs_test)\n",
    "\n",
    "dataset_7 =  WordEmbeddingDataset(temp_7_train, temp_7_targs_train)\n",
    "dataset_7_test = WordEmbeddingDataset(temp_7_test, temp_7_targs_test)\n",
    "\n",
    "dataset_14 = WordEmbeddingDataset(temp_14_train, temp_14_targs_train)\n",
    "dataset_14_test = WordEmbeddingDataset(temp_14_test, temp_14_targs_test)\n",
    "\n",
    "dataset_all = WordEmbeddingDataset(temp_all_train, temp_all_targs_train)\n",
    "dataset_all_test = WordEmbeddingDataset(temp_all_test, temp_all_targs_test)\n",
    "\n",
    "data_loader_0 = DataLoader(dataset_0, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_0_test = DataLoader(dataset_0_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_7 = DataLoader(dataset_7, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_7_test = DataLoader(dataset_7_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_14 = DataLoader(dataset_14, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_14_test = DataLoader(dataset_14_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_all = DataLoader(dataset_all, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_all_test = DataLoader(dataset_all_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5360c31d-5170-4649-a1c8-962234458f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(fa_module, proj_module, supconloss_module, classifier, data_loader, optimizer, classifier_loss_fn):\n",
    "    fa_module.train()\n",
    "    proj_module.train()\n",
    "    supconloss_module.train()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_acc_cumulative = 0\n",
    "    n_batches = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for _, data in tqdm(enumerate(data_loader)):\n",
    "        n_batches += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cls_embs = data[0].squeeze(1)  # Assuming BERT CLS embeddings\n",
    "        targets = data[1]\n",
    "\n",
    "        # Forward pass through feature extractor and projection modules\n",
    "        fam_output = fa_module(cls_embs)   \n",
    "        proj_output = proj_module(fam_output)\n",
    "\n",
    "        # Calculate the SupConLoss1 (replace SupConLoss here)\n",
    "        supcon_loss = supconloss_module(proj_output, targets)\n",
    "\n",
    "        # Forward pass through the classifier\n",
    "        classifier_output = classifier(fam_output)  \n",
    "        classifier_loss = classifier_loss_fn(classifier_output, targets)\n",
    "\n",
    "        # Combine the SupCon loss and the classifier loss\n",
    "        loss = supcon_loss + classifier_loss \n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the cumulative loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        batch_predictions = classifier_output.argmax(1)\n",
    "        batch_acc = (batch_predictions == targets).sum().item() / len(targets)\n",
    "        batch_acc_cumulative += batch_acc\n",
    "\n",
    "    average_acc = batch_acc_cumulative / n_batches\n",
    "    average_loss = train_loss / n_batches\n",
    "\n",
    "    return average_loss, average_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8090575-dc20-4bb2-8897-8805cce71d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(fa_module, classifier, data_loader):\n",
    "    fa_module.eval()  \n",
    "    classifier.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  \n",
    "        for data in data_loader:\n",
    "            cls_embs = data[0].squeeze(1)  \n",
    "            targets = data[1].tolist()\n",
    "            \n",
    "            fam_output = fa_module(cls_embs)\n",
    "            final_output = classifier(fam_output)\n",
    "            preds = final_output.argmax(1).tolist()\n",
    "            \n",
    "            total += len(preds) \n",
    "            correct += np.sum(np.array(preds) == np.array(targets))  \n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e41d0-dafd-44ca-b4b8-77d333d6bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_0 = FAM(797, 256, 0.3)\n",
    "proj_0 = Projection(256, 128)\n",
    "supcon_0 = SupConLoss()\n",
    "classifier_0 = Classifier(256, 3, 0.3)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(fam_0.parameters()) + \n",
    "                             list(proj_0.parameters()) + \n",
    "                             list(classifier_0.parameters()), lr=0.001)\n",
    "classifier_loss = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "i = 0\n",
    "best_acc = 0\n",
    "while i <10:\n",
    "    loss, acc = train(fam_0, proj_0, supcon_0, classifier_0, data_loader_0, optimizer, classifier_loss)  \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_fam_0 = fam_0.state_dict()  \n",
    "        best_proj_0 = proj_0.state_dict() \n",
    "        best_classifier = classifier_0.state_dict()\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "    scheduler.step()\n",
    "    \n",
    "fam_0.load_state_dict(best_fam_0)\n",
    "proj_0.load_state_dict(best_proj_0)\n",
    "classifier_0.load_state_dict(best_classifier)\n",
    "test_accuracy = evaluate(fam_0, classifier_0, data_loader_0_test)\n",
    "print('Test Set Accuracy: ' + str(test_accuracy*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85f8f0-c25a-4c1f-83fb-58fcc407fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_7 = FAM(797, 256, 0.3)\n",
    "proj_7 = Projection(256, 128)\n",
    "supcon_7 = SupConLoss()\n",
    "classifier_7 = Classifier(256, 3, 0.3)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(fam_7.parameters()) + \n",
    "                             list(proj_7.parameters()) + \n",
    "                             list(classifier_7.parameters()), lr=0.001)\n",
    "classifier_loss = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "i = 0\n",
    "best_acc = 0\n",
    "while i <10:\n",
    "    loss, acc = train(fam_7, proj_7, supcon_7, classifier_7, data_loader_7, optimizer, classifier_loss)  \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_fam_7 = fam_7.state_dict()  \n",
    "        best_proj_7 = proj_7.state_dict() \n",
    "        best_classifier_7 = classifier_7.state_dict()\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "    scheduler.step()\n",
    "    \n",
    "fam_7.load_state_dict(best_fam_7)\n",
    "proj_7.load_state_dict(best_proj_7)\n",
    "classifier_7.load_state_dict(best_classifier_7)\n",
    "test_accuracy = evaluate(fam_7, classifier_7, data_loader_7_test)\n",
    "print('Test Set Accuracy: ' + str(test_accuracy*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc19da-d2c9-4085-910a-83d8b750ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_14 = FAM(797, 256, 0.3)\n",
    "proj_14 = Projection(256, 128)\n",
    "supcon_14 = SupConLoss()\n",
    "classifier_14 = Classifier(256, 3, 0.3)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(fam_0.parameters()) + \n",
    "                             list(proj_0.parameters()) + \n",
    "                             list(classifier_0.parameters()), lr=0.001)\n",
    "classifier_loss = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "i = 0\n",
    "best_acc = 0\n",
    "while i <10:\n",
    "    loss, acc = train(fam_14, proj_14, supcon_14, classifier_14, data_loader_14, optimizer, classifier_loss)  \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_fam_14 = fam_14.state_dict()  \n",
    "        best_proj_14 = proj_14.state_dict() \n",
    "        best_classifier_14 = classifier_14.state_dict()\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "    scheduler.step()\n",
    "    \n",
    "fam_14.load_state_dict(best_fam_14)\n",
    "proj_14.load_state_dict(best_proj_14)\n",
    "classifier_14.load_state_dict(best_classifier)\n",
    "test_accuracy = evaluate(fam_14, classifier_14, data_loader_14_test)\n",
    "print('Test Set Accuracy: ' + str(test_accuracy*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e8bbf-59a2-492f-b222-fbdaa3a2e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for training on all temperatures\n",
    "fam_all = FAM(768, 256, 0.3)\n",
    "proj_all = Projection(256, 128)\n",
    "supcon_all = SupConLoss()\n",
    "classifier_all = Classifier(256, 3, 0.3)\n",
    "optimizer = torch.optim.Adam(list(fam_all.parameters()) + \n",
    "                             list(proj_all.parameters()) + \n",
    "                             list(supcon_all.parameters()) + \n",
    "                             list(classifier_all.parameters()), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "for epoch in range(1, 20):\n",
    "    loss, acc = train(fam_all, proj_all, supcon_all, classifier_all, data_loader_all)  \n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "    scheduler.step()\n",
    "test_accuracy = evaluate(fam_all, proj_all, supcon_all, classifier_all, data_loader_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e4158-694d-4ad3-a1c7-2a420fca4256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
