{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4407368a-2d64-475f-b906-527d05c88a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29cfe03-c329-4e9b-b3c7-dfe82fa57f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cls_emb.pkl', 'rb') as f:\n",
    "    cls = pickle.load(f)\n",
    "with open('feature_vectors.pkl', 'rb')as f:\n",
    "    feature_vectors= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1194a9a-fa95-4076-9047-94d14d2e4c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a0b872-1ef6-4c21-98d9-38541d968211",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv('final_data.csv')\n",
    "map_dict = {'llama3.1-70b':0, 'mistral':1, 'gpt-4o-2024-05-13':2}\n",
    "response_df['model_nums'] = response_df['model'].map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928db930-1efe-410a-a3b4-691ced778826",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [torch.cat((cls[i].float(), torch.from_numpy(feature_vectors[i]).unsqueeze(0).float()), dim=1) for i in range(len(cls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb05e48-b291-4de9-a164-b3360f7ed5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_split(response_df, embeddings, temperature):\n",
    "    temp_idx = response_df[response_df['temperature'] == temperature].index\n",
    "    temp_embs = [embeddings[idx] for idx in temp_idx]\n",
    "    temp_targs = [response_df['model_nums'][idx] for idx in temp_idx]\n",
    "    \n",
    "    return train_test_split(temp_embs, temp_targs, test_size=0.1, random_state=42)\n",
    "    \n",
    "temp_0_train, temp_0_test, temp_0_targs_train, temp_0_targs_test = extract_and_split(response_df, embeddings, 0)\n",
    "temp_7_train, temp_7_test, temp_7_targs_train, temp_7_targs_test = extract_and_split(response_df, embeddings, 0.7)\n",
    "temp_14_train, temp_14_test, temp_14_targs_train, temp_14_targs_test = extract_and_split(response_df, embeddings, 1.4)\n",
    "temp_all_train, temp_all_test, temp_all_targs_train, temp_all_targs_test = train_test_split(embeddings, response_df['model_nums'], \n",
    "                                                                                            test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63705575-6ec2-4d28-b3e4-1f45c0226f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(embed_size, hidden_size)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.2\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch,  dim = text.size()\n",
    "        feat = self.fc(torch.tanh(self.dropout(text.view(batch, dim))))\n",
    "        feat = F.normalize(feat, dim=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd52bf4-0134-4e2d-9d0f-5b2db6c63d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, hidden_size, projection_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size, projection_size)\n",
    "        self.ln = nn.LayerNorm(projection_size)\n",
    "        self.bn = nn.BatchNorm1d(projection_size)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.01\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch,  dim = text.size()\n",
    "        return self.ln(self.fc(torch.tanh(text.view(batch, dim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ee1f5b4-904a-4b3f-92cb-99dda016097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Implementation of the loss described in the paper Supervised Contrastive Learning :\n",
    "        https://arxiv.org/abs/2004.11362\n",
    "\n",
    "        :param temperature: int\n",
    "        \"\"\"\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, projections, targets):\n",
    "        \"\"\"\n",
    "\n",
    "        :param projections: torch.Tensor, shape [batch_size, projection_dim]\n",
    "        :param targets: torch.Tensor, shape [batch_size]\n",
    "        :return: torch.Tensor, scalar\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\") if projections.is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "        dot_product_tempered = torch.mm(projections, projections.T) / self.temperature\n",
    "        # Minus max for numerical stability with exponential. Same done in cross entropy. Epsilon added to avoid log(0)\n",
    "        exp_dot_tempered = (\n",
    "            torch.exp(dot_product_tempered - torch.max(dot_product_tempered, dim=1, keepdim=True)[0]) + 1e-5\n",
    "        )\n",
    "\n",
    "        mask_similar_class = (targets.unsqueeze(1).repeat(1, targets.shape[0]) == targets).to(device)\n",
    "        mask_anchor_out = (1 - torch.eye(exp_dot_tempered.shape[0])).to(device)\n",
    "        mask_combined = mask_similar_class * mask_anchor_out\n",
    "        cardinality_per_samples = torch.sum(mask_combined, dim=1)\n",
    "\n",
    "        log_prob = -torch.log(exp_dot_tempered / (torch.sum(exp_dot_tempered * mask_anchor_out, dim=1, keepdim=True)))\n",
    "        supervised_contrastive_loss_per_sample = torch.sum(log_prob * mask_combined, dim=1) / cardinality_per_samples\n",
    "        supervised_contrastive_loss = torch.mean(supervised_contrastive_loss_per_sample)\n",
    "\n",
    "        return supervised_contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc98701-4433-49ab-9760-763b214973d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_class, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.02\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        return self.fc(torch.tanh(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d8db8b-6f5d-43b6-8637-7e0da9755196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, cls_embs, targs):\n",
    "        self.cls_embs = cls_embs\n",
    "        self.targs = targs \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cls_embs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cls_embs[idx], self.targs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c91b276-e20c-438c-bf94-3c4dbdd3c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "dataset_0 = WordEmbeddingDataset(temp_0_train, temp_0_targs_train)\n",
    "dataset_0_test = WordEmbeddingDataset(temp_0_test, temp_0_targs_test)\n",
    "\n",
    "dataset_7 =  WordEmbeddingDataset(temp_7_train, temp_7_targs_train)\n",
    "dataset_7_test = WordEmbeddingDataset(temp_7_test, temp_7_targs_test)\n",
    "\n",
    "dataset_14 = WordEmbeddingDataset(temp_14_train, temp_14_targs_train)\n",
    "dataset_14_test = WordEmbeddingDataset(temp_14_test, temp_14_targs_test)\n",
    "\n",
    "dataset_all = WordEmbeddingDataset(temp_all_train, temp_all_targs_train)\n",
    "dataset_all_test = WordEmbeddingDataset(temp_all_test, temp_all_targs_test)\n",
    "\n",
    "data_loader_0 = DataLoader(dataset_0, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_0_test = DataLoader(dataset_0_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_7 = DataLoader(dataset_7, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_7_test = DataLoader(dataset_7_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_14 = DataLoader(dataset_14, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_14_test = DataLoader(dataset_14_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_all = DataLoader(dataset_all, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_all_test = DataLoader(dataset_all_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5360c31d-5170-4649-a1c8-962234458f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fa_module, proj_module, supconloss_module, classifier, data_loader, optimizer, classifier_loss_fn):\n",
    "    fa_module.train()\n",
    "    proj_module.train()\n",
    "    supconloss_module.train()\n",
    "    classifier.train()\n",
    "\n",
    "    batch_acc_cumulative = 0\n",
    "    n_batches = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for _, data in tqdm(enumerate(data_loader)):\n",
    "        n_batches += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        cls_embs = data[0].squeeze(1)  # Assuming BERT CLS embeddings\n",
    "        targets = data[1]\n",
    "        \n",
    "        fam_output = fa_module(cls_embs)   \n",
    "        proj_output = proj_module(fam_output)\n",
    "        supcon_loss = supconloss_module(proj_output, targets)\n",
    "        classifier_output = classifier(fam_output)  \n",
    "        classifier_loss = classifier_loss_fn(classifier_output, targets)\n",
    "\n",
    "        loss = supcon_loss + classifier_loss \n",
    "\n",
    "        loss.backward()   # Backpropagate the combined loss\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        \n",
    "        batch_predictions = classifier_output.argmax(1)\n",
    "        batch_acc = (batch_predictions == targets).sum().item() / len(targets)\n",
    "        batch_acc_cumulative += batch_acc\n",
    "\n",
    "    \n",
    "    average_acc = batch_acc_cumulative / n_batches\n",
    "    average_loss = train_loss / n_batches\n",
    "\n",
    "    return average_loss, average_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8090575-dc20-4bb2-8897-8805cce71d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(fa_module, classifier, data_loader):\n",
    "    fa_module.eval()  \n",
    "    classifier.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  \n",
    "        for data in data_loader:\n",
    "            cls_embs = data[0].squeeze(1)  \n",
    "            targets = data[1].tolist()\n",
    "            \n",
    "            fam_output = fa_module(cls_embs)\n",
    "            final_output = classifier(fam_output)\n",
    "            preds = final_output.argmax(1).tolist()\n",
    "            \n",
    "            total += len(preds) \n",
    "            correct += np.sum(np.array(preds) == np.array(targets))  \n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "750e41d0-dafd-44ca-b4b8-77d333d6bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 157.46it/s]\n",
      "43it [00:00, 158.70it/s]\n",
      "43it [00:00, 157.27it/s]\n",
      "43it [00:00, 141.73it/s]\n",
      "43it [00:00, 153.47it/s]\n",
      "43it [00:00, 144.62it/s]\n",
      "43it [00:00, 136.24it/s]\n",
      "43it [00:00, 165.46it/s]\n",
      "43it [00:00, 162.80it/s]\n",
      "43it [00:00, 176.59it/s]\n",
      "43it [00:00, 159.49it/s]\n",
      "43it [00:00, 160.17it/s]\n",
      "43it [00:00, 151.69it/s]\n",
      "43it [00:00, 165.72it/s]\n",
      "43it [00:00, 168.00it/s]\n",
      "43it [00:00, 169.35it/s]\n",
      "43it [00:00, 145.31it/s]\n",
      "43it [00:00, 139.84it/s]\n",
      "43it [00:00, 165.33it/s]\n",
      "43it [00:00, 165.54it/s]\n",
      "43it [00:00, 166.37it/s]\n",
      "43it [00:00, 167.02it/s]\n",
      "43it [00:00, 166.96it/s]\n",
      "43it [00:00, 156.09it/s]\n",
      "43it [00:00, 160.11it/s]\n",
      "43it [00:00, 162.59it/s]\n",
      "43it [00:00, 134.18it/s]\n",
      "43it [00:00, 164.33it/s]\n",
      "43it [00:00, 164.45it/s]\n",
      "43it [00:00, 150.80it/s]\n",
      "43it [00:00, 163.90it/s]\n",
      "43it [00:00, 164.05it/s]\n",
      "43it [00:00, 156.70it/s]\n",
      "43it [00:00, 152.94it/s]\n",
      "43it [00:00, 167.08it/s]\n",
      "43it [00:00, 154.46it/s]\n",
      "43it [00:00, 138.93it/s]\n",
      "43it [00:00, 134.40it/s]\n",
      "43it [00:00, 114.16it/s]\n",
      "43it [00:00, 104.18it/s]\n",
      "43it [00:00, 107.85it/s]\n",
      "43it [00:00, 104.28it/s]\n",
      "43it [00:00, 100.89it/s]\n",
      "43it [00:00, 100.11it/s]\n",
      "43it [00:00, 108.27it/s]\n",
      "43it [00:00, 107.30it/s]\n",
      "43it [00:00, 111.30it/s]\n",
      "43it [00:00, 113.83it/s]\n",
      "43it [00:00, 104.39it/s]\n",
      "43it [00:00, 107.93it/s]\n",
      "43it [00:00, 102.54it/s]\n",
      "43it [00:00, 110.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 56.49999999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fam_0 = FAM(797, 256, 0.3)\n",
    "proj_0 = Projection(256, 128)\n",
    "supcon_0 = SupConLoss()\n",
    "classifier_0 = Classifier(256, 3, 0.3)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(fam_0.parameters()) + \n",
    "                             list(proj_0.parameters()) + \n",
    "                             list(classifier_0.parameters()), lr=0.001)\n",
    "classifier_loss = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "i = 0\n",
    "best_acc = 0\n",
    "while i <10:\n",
    "    loss, acc = train(fam_0, proj_0, supcon_0, classifier_0, data_loader_0, optimizer, classifier_loss)  \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_fam_0 = fam_0.state_dict()  \n",
    "        best_proj_0 = proj_0.state_dict() \n",
    "        best_classifier = classifier_0.state_dict()\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "    scheduler.step()\n",
    "    \n",
    "fam_0.load_state_dict(best_fam_0)\n",
    "proj_0.load_state_dict(best_proj_0)\n",
    "classifier_0.load_state_dict(best_classifier)\n",
    "test_accuracy = evaluate(fam_0, classifier_0, data_loader_0_test)\n",
    "print('Test Set Accuracy: ' + str(test_accuracy*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc85f8f0-c25a-4c1f-83fb-58fcc407fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 153.33it/s]\n",
      "43it [00:00, 151.79it/s]\n",
      "43it [00:00, 162.24it/s]\n",
      "43it [00:00, 171.48it/s]\n",
      "43it [00:00, 163.59it/s]\n",
      "43it [00:00, 169.59it/s]\n",
      "43it [00:00, 164.86it/s]\n",
      "43it [00:00, 166.03it/s]\n",
      "43it [00:00, 170.35it/s]\n",
      "43it [00:00, 162.35it/s]\n",
      "43it [00:00, 174.50it/s]\n",
      "43it [00:00, 166.93it/s]\n",
      "43it [00:00, 171.05it/s]\n",
      "43it [00:00, 174.76it/s]\n",
      "43it [00:00, 169.61it/s]\n",
      "43it [00:00, 168.98it/s]\n",
      "43it [00:00, 161.31it/s]\n",
      "43it [00:00, 145.84it/s]\n",
      "43it [00:00, 162.72it/s]\n",
      "43it [00:00, 166.35it/s]\n",
      "43it [00:00, 158.44it/s]\n",
      "43it [00:00, 131.81it/s]\n",
      "43it [00:00, 142.90it/s]\n",
      "43it [00:00, 160.39it/s]\n",
      "43it [00:00, 150.07it/s]\n",
      "43it [00:00, 163.57it/s]\n",
      "43it [00:00, 141.38it/s]\n",
      "43it [00:00, 162.87it/s]\n",
      "43it [00:00, 169.64it/s]\n",
      "43it [00:00, 168.60it/s]\n",
      "43it [00:00, 171.46it/s]\n",
      "43it [00:00, 169.90it/s]\n",
      "43it [00:00, 153.80it/s]\n",
      "43it [00:00, 138.40it/s]\n",
      "43it [00:00, 167.68it/s]\n",
      "43it [00:00, 168.66it/s]\n",
      "43it [00:00, 155.46it/s]\n",
      "43it [00:00, 147.06it/s]\n",
      "43it [00:00, 135.46it/s]\n",
      "43it [00:00, 148.49it/s]\n",
      "43it [00:00, 165.68it/s]\n",
      "43it [00:00, 162.87it/s]\n",
      "43it [00:00, 137.49it/s]\n",
      "43it [00:00, 111.16it/s]\n",
      "43it [00:00, 111.08it/s]\n",
      "43it [00:00, 111.54it/s]\n",
      "43it [00:00, 98.33it/s] \n",
      "43it [00:00, 101.03it/s]\n",
      "43it [00:00, 98.91it/s] \n",
      "43it [00:00, 111.98it/s]\n",
      "43it [00:00, 112.06it/s]\n",
      "43it [00:00, 109.70it/s]\n",
      "43it [00:00, 100.37it/s]\n",
      "43it [00:00, 109.80it/s]\n",
      "43it [00:00, 107.97it/s]\n",
      "43it [00:00, 105.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 53.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fam_7 = FAM(797, 256, 0.3)\n",
    "proj_7 = Projection(256, 128)\n",
    "supcon_7 = SupConLoss()\n",
    "classifier_7 = Classifier(256, 3, 0.3)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(fam_7.parameters()) + \n",
    "                             list(proj_7.parameters()) + \n",
    "                             list(classifier_7.parameters()), lr=0.001)\n",
    "classifier_loss = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "i = 0\n",
    "best_acc = 0\n",
    "while i <10:\n",
    "    loss, acc = train(fam_7, proj_7, supcon_7, classifier_7, data_loader_7, optimizer, classifier_loss)  \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_fam_7 = fam_7.state_dict()  \n",
    "        best_proj_7 = proj_7.state_dict() \n",
    "        best_classifier_7 = classifier_7.state_dict()\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "    scheduler.step()\n",
    "    \n",
    "fam_7.load_state_dict(best_fam_7)\n",
    "proj_7.load_state_dict(best_proj_7)\n",
    "classifier_7.load_state_dict(best_classifier_7)\n",
    "test_accuracy = evaluate(fam_7, classifier_7, data_loader_7_test)\n",
    "print('Test Set Accuracy: ' + str(test_accuracy*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc19da-d2c9-4085-910a-83d8b750ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_14 = FAM(797, 256, 0.3)\n",
    "proj_14 = Projection(256, 128)\n",
    "supcon_14 = SupConLoss()\n",
    "classifier_14 = Classifier(256, 3, 0.3)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(fam_0.parameters()) + \n",
    "                             list(proj_0.parameters()) + \n",
    "                             list(classifier_0.parameters()), lr=0.001)\n",
    "classifier_loss = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "i = 0\n",
    "best_acc = 0\n",
    "while i <10:\n",
    "    loss, acc = train(fam_14, proj_14, supcon_14, classifier_14, data_loader_14, optimizer, classifier_loss)  \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_fam_14 = fam_14.state_dict()  \n",
    "        best_proj_14 = proj_14.state_dict() \n",
    "        best_classifier_14 = classifier_14.state_dict()\n",
    "        i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "    scheduler.step()\n",
    "    \n",
    "fam_14.load_state_dict(best_fam_14)\n",
    "proj_14.load_state_dict(best_proj_14)\n",
    "classifier_14.load_state_dict(best_classifier)\n",
    "test_accuracy = evaluate(fam_14, classifier_14, data_loader_14_test)\n",
    "print('Test Set Accuracy: ' + str(test_accuracy*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e8bbf-59a2-492f-b222-fbdaa3a2e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_all = FAM(768, 256, 0.3)\n",
    "proj_all = Projection(256, 128)\n",
    "supcon_all = SupConHead()\n",
    "classifier_all = Classifier(256, 3, 0.3)\n",
    "optimizer = torch.optim.Adam(list(fam_all.parameters()) + \n",
    "                             list(proj_all.parameters()) + \n",
    "                             list(supcon_all.parameters()) + \n",
    "                             list(classifier_all.parameters()), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "for epoch in range(1, 20):\n",
    "    loss, acc = train(fam_all, proj_all, supcon_all, classifier_all, data_loader_all)  \n",
    "    print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "    scheduler.step()\n",
    "test_accuracy = evaluate(fam_all, proj_all, supcon_all, classifier_all, data_loader_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e4158-694d-4ad3-a1c7-2a420fca4256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
