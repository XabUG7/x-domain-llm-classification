{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d92b2816-3514-46f2-b4a0-f30f03e0dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd60d8fd-4699-4b7a-aa40-f31db45c5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bdd070-daa0-4f51-baf6-501aaa86a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df['stylometry_vector'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf02db00-dfdc-4e82-ab1f-696b623c78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44502aa6-8dac-4ae1-a118-829001d26072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hapax_legomena(df):\n",
    "    \n",
    "    total_word_counts = Counter()\n",
    "    \n",
    "    for response in df['response_text']:\n",
    "        words = response.split()  \n",
    "        total_word_counts.update(words) \n",
    "    \n",
    "    hapax_legomena = {word for word, count in total_word_counts.items() if count == 1}\n",
    "    \n",
    "    return hapax_legomena\n",
    "hapax_legomena_dict = {word:0 for word in get_hapax_legomena(response_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18ab8f5e-8421-4fcd-85db-9928dfcca0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_vector(response):\n",
    "\n",
    "    # Get average word length\n",
    "    num_words = len(response.split())\n",
    "    running_total = sum(len(word) for word in response.split())\n",
    "    average_word_len = running_total/num_words\n",
    "\n",
    "    # Get function word usage, stopword usage, active/passive, and present/past\n",
    "    pos_list = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \n",
    "                \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\", \"SPACE\"]\n",
    "    pos_dict = {pos:0 for pos in pos_list}\n",
    "    doc = nlp(response)\n",
    "    num_tokens = len(doc)\n",
    "    num_stopwords = 0\n",
    "    active = 1\n",
    "    present = 1\n",
    "    for token in doc:\n",
    "        type_ = token.pos_\n",
    "        pos_dict[type_] += 1\n",
    "        if token.is_stop:\n",
    "            num_stopwords += 1\n",
    "        if token.dep_ =='nsubjpass':\n",
    "            active = 0\n",
    "        if type_ == \"VERB\":\n",
    "            if token.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                present = 0\n",
    "    stopword_ratio = num_stopwords/num_tokens\n",
    "    \n",
    "    # Get MTTR\n",
    "    pointer_left = 0\n",
    "    pointer_right = 4\n",
    "    TTRs = []\n",
    "    while pointer_right < len(doc):\n",
    "        text_window = doc[pointer_left:pointer_right]\n",
    "        TTRs.append(len(set(text_window))/len(text_window))\n",
    "        pointer_left += 1\n",
    "        pointer_right += 1\n",
    "    MTTR = np.mean(TTRs)\n",
    "    \n",
    "    # Get Hapax Legomena\n",
    "    #hapax_dict = hapax_legomena_dict.copy()\n",
    "    #for word in response.split():\n",
    "     #   if word in hapax_dict.keys():\n",
    "      #      hapax_dict[word] += 1\n",
    "    \n",
    "    pos_vals = np.array(list(pos_dict.values()))\n",
    "    #hapax_legomana_vals = np.array(list(hapax_dict.values()))\n",
    "    lexical_vector = np.concatenate(([average_word_len, stopword_ratio, MTTR], pos_vals))\n",
    "\n",
    "    # Get syntatic_features\n",
    "\n",
    "    # Get average sentence length\n",
    "    num_sents = len([sent for sent in doc.sents])\n",
    "    num_words = len([token for token in doc if not token.is_punct])\n",
    "    avg_word_per_sent = num_words/num_sents\n",
    "\n",
    "    syntactic_vector = np.array([avg_word_per_sent, active, present])\n",
    "\n",
    "    # Get structural features\n",
    "    paragraphs = response.split('\\n\\n')\n",
    "    num_paragraphs = len(paragraphs)\n",
    "    sentences_per_paragraph = num_sents/num_paragraphs\n",
    "    words_per_paragraph = num_words/num_paragraphs\n",
    "\n",
    "    num_puncts = len([token for token in doc if token.is_punct])\n",
    "    puncts_per_word = num_puncts/num_words\n",
    "\n",
    "    num_capitals = len([token for token in doc if token.text[0].isupper()])\n",
    "    capitals_per_word = num_capitals/num_words\n",
    "\n",
    "    structural_vector = np.array((sentences_per_paragraph, words_per_paragraph, puncts_per_word, capitals_per_word))\n",
    "    \n",
    "    final_vector = np.concatenate([lexical_vector, syntactic_vector, structural_vector])\n",
    "    norm = np.linalg.norm(final_vector)\n",
    "    final_vector_normalized = final_vector/norm\n",
    "    return final_vector_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e568ba8-e3f8-4c26-8d50-f417bde083ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14637it [08:23, 29.09it/s] \n"
     ]
    }
   ],
   "source": [
    "feature_vectors = []\n",
    "for i, row in tqdm(response_df.iterrows()):\n",
    "    response = row['response_text']\n",
    "    feature_vector = get_features_vector(response)\n",
    "    feature_vectors.append(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65d6fcb5-920c-4393-9f4e-74ea615f67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "17fce77e-172c-4f42-b7b5-8a519382ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982b641-d522-4669-a991-2efa92be7151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
