{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4407368a-2d64-475f-b906-527d05c88a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b29cfe03-c329-4e9b-b3c7-dfe82fa57f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-computed BERT CLS vectors and Feature Vectors\n",
    "with open('cls_emb.pkl', 'rb') as f:\n",
    "    cls = pickle.load(f)\n",
    "with open('feature_vectors.pkl', 'rb')as f:\n",
    "    feature_vectors= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2a0b872-1ef6-4c21-98d9-38541d968211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model needs numerical classes as base truth targets. Convert model names to 0, 1, 2\n",
    "response_df = pd.read_csv('final_data.csv')\n",
    "map_dict = {'llama3.1-70b':0, 'mistral':1, 'gpt-4o-2024-05-13':2}\n",
    "response_df['model_nums'] = response_df['model'].map(map_dict)\n",
    "model_nums = response_df['model_nums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "928db930-1efe-410a-a3b4-691ced778826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the CLS and feature vectors\n",
    "embeddings = [torch.cat((cls[i].float(), torch.from_numpy(feature_vectors[i]).unsqueeze(0).float()), dim=1) for i in range(len(cls))]\n",
    "\n",
    "# Calculate the number of features to be used later as input to model\n",
    "NUM_FEATURES = embeddings[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e58f52e-c24f-4385-9ccc-2b21bf5135df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a development and validation set by splitting indices\n",
    "RANDOM_STATE = 42\n",
    "indices = [i for i in range(len(embeddings))]\n",
    "dev_indices, val_indices = train_test_split(indices, test_size = 0.1, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7cb05e48-b291-4de9-a164-b3360f7ed5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_split_dev(temp, dev=True):\n",
    "    \"\"\"\n",
    "    Returns split train, test, and dev sets based on temperature\n",
    "\n",
    "    temp: float indicating temperature \n",
    "    dev: boolean indicating if the return set is the dev set or validation\n",
    "    \"\"\"\n",
    "    if dev:\n",
    "        temp_embs = [embeddings[idx] for idx in dev_indices if response_df['temperature'][idx]==temp]\n",
    "        temp_targs = [model_nums[idx] for idx in dev_indices if response_df['temperature'][idx]==temp]\n",
    "        return train_test_split(temp_embs, temp_targs, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    if not dev:\n",
    "        return ([embeddings[idx] for idx in val_indices if response_df['temperature'][idx]==temp], \n",
    "            [model_nums[idx] for idx in val_indices if response_df['temperature'][idx]==temp])\n",
    "\n",
    "temp_0_train, temp_0_test, temp_0_targs_train, temp_0_targs_test = extract_and_split_dev(0)\n",
    "temp_7_train, temp_7_test, temp_7_targs_train, temp_7_targs_test = extract_and_split_dev(0.7)\n",
    "temp_14_train, temp_14_test, temp_14_targs_train, temp_14_targs_test = extract_and_split_dev(1.4)\n",
    "temp_0_val, temp_0_val_targs = extract_and_split_dev(0, False)\n",
    "temp_7_val, temp_7_val_targs = extract_and_split_dev(0.7, False)\n",
    "temp_14_val, temp_14_val_targs = extract_and_split_dev(1.4, False)\n",
    "temp_all_val, temp_all_val_targs = [embeddings[idx] for idx in val_indices],[model_nums[idx] for idx in val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66b64b50-be64-406d-a307-1f9fc5d34819",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_all_embs = [embeddings[idx] for idx in dev_indices]\n",
    "temp_all_targs = [model_nums[idx] for idx in dev_indices]\n",
    "temp_all_train, temp_all_test, temp_all_targs_train, temp_all_targs_test = train_test_split(\n",
    "    temp_all_embs, temp_all_targs, test_size = 0.2, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5f2de10f-9338-40e6-926a-cc2a48185295",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_0_val.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_0_val, f)\n",
    "with open('temp_0_val_targs.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_0_val_targs, f)\n",
    "\n",
    "with open('temp_7_val.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_7_val, f)\n",
    "with open('temp_7_val_targs.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_7_val_targs, f)\n",
    "\n",
    "with open('temp_14_val.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_14_val, f)\n",
    "with open('temp_14_val_targs.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_14_val_targs, f)\n",
    "\n",
    "with open('temp_all_val.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_all_val, f)\n",
    "with open('temp_all_val_targs.pkl', 'wb') as f:\n",
    "    pickle.dump(temp_all_val_targs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "63705575-6ec2-4d28-b3e4-1f45c0226f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(embed_size, hidden_size)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.2\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch,  dim = text.size()\n",
    "        feat = self.fc(torch.tanh(self.dropout(text.view(batch, dim))))\n",
    "        feat = F.normalize(feat, dim=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1bd52bf4-0134-4e2d-9d0f-5b2db6c63d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, hidden_size, projection_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size, projection_size)\n",
    "        self.ln = nn.LayerNorm(projection_size)\n",
    "        self.bn = nn.BatchNorm1d(projection_size)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.01\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch,  dim = text.size()\n",
    "        return self.ln(self.fc(torch.tanh(text.view(batch, dim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ee1f5b4-904a-4b3f-92cb-99dda016097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Implementation of the loss described in the paper Supervised Contrastive Learning :\n",
    "        https://arxiv.org/abs/2004.11362\n",
    "\n",
    "        :param temperature: int\n",
    "        \"\"\"\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, projections, targets):\n",
    "        \"\"\"\n",
    "\n",
    "        :param projections: torch.Tensor, shape [batch_size, projection_dim]\n",
    "        :param targets: torch.Tensor, shape [batch_size]\n",
    "        :return: torch.Tensor, scalar\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\") if projections.is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "        dot_product_tempered = torch.mm(projections, projections.T) / self.temperature\n",
    "        # Minus max for numerical stability with exponential. Same done in cross entropy. Epsilon added to avoid log(0)\n",
    "        exp_dot_tempered = (\n",
    "            torch.exp(dot_product_tempered - torch.max(dot_product_tempered, dim=1, keepdim=True)[0]) + 1e-5\n",
    "        )\n",
    "\n",
    "        mask_similar_class = (targets.unsqueeze(1).repeat(1, targets.shape[0]) == targets).to(device)\n",
    "        mask_anchor_out = (1 - torch.eye(exp_dot_tempered.shape[0])).to(device)\n",
    "        mask_combined = mask_similar_class * mask_anchor_out\n",
    "        cardinality_per_samples = torch.sum(mask_combined, dim=1)\n",
    "\n",
    "        log_prob = -torch.log(exp_dot_tempered / (torch.sum(exp_dot_tempered * mask_anchor_out, dim=1, keepdim=True)))\n",
    "        supervised_contrastive_loss_per_sample = torch.sum(log_prob * mask_combined, dim=1) / cardinality_per_samples\n",
    "        supervised_contrastive_loss = torch.mean(supervised_contrastive_loss_per_sample)\n",
    "\n",
    "        return supervised_contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6bc98701-4433-49ab-9760-763b214973d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_class, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.02\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        return self.fc(torch.tanh(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83d8db8b-6f5d-43b6-8637-7e0da9755196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset object to feed into the dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, embs, targs):\n",
    "        \"\"\"\n",
    "        Class initializer\n",
    "\n",
    "        embs: concatenated CLS embeddings and feature vectors\n",
    "        targs: base truth model numbers\n",
    "        \"\"\"\n",
    "        self.embs = embs\n",
    "        self.targs = targs \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Method used by data loader to output data\n",
    "        \n",
    "        idx: random batch index from dataloader\n",
    "\n",
    "        Returns: \n",
    "        Tuple with batch embeddings at [0] and batch base truth targets at [1]\n",
    "        \"\"\"\n",
    "        return self.embs[idx], self.targs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c91b276-e20c-438c-bf94-3c4dbdd3c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch size of 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Create datasets to be used in data loaders\n",
    "dataset_0 = WordEmbeddingDataset(temp_0_train, temp_0_targs_train)\n",
    "dataset_0_test = WordEmbeddingDataset(temp_0_test, temp_0_targs_test)\n",
    "\n",
    "dataset_7 =  WordEmbeddingDataset(temp_7_train, temp_7_targs_train)\n",
    "dataset_7_test = WordEmbeddingDataset(temp_7_test, temp_7_targs_test)\n",
    "\n",
    "dataset_14 = WordEmbeddingDataset(temp_14_train, temp_14_targs_train)\n",
    "dataset_14_test = WordEmbeddingDataset(temp_14_test, temp_14_targs_test)\n",
    "\n",
    "dataset_all = WordEmbeddingDataset(temp_all_train, temp_all_targs_train)\n",
    "dataset_all_test = WordEmbeddingDataset(temp_all_test, temp_all_targs_test)\n",
    "\n",
    "# Create all dataloaders\n",
    "data_loader_0_train = DataLoader(dataset_0, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_0_test = DataLoader(dataset_0_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_7_train = DataLoader(dataset_7, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_7_test = DataLoader(dataset_7_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_14_train = DataLoader(dataset_14, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_14_test = DataLoader(dataset_14_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_all_train = DataLoader(dataset_all, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_all_test = DataLoader(dataset_all_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "750e41d0-dafd-44ca-b4b8-77d333d6bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGenerator():\n",
    "    \"\"\"\n",
    "    Because there are dropout layers in the networks, each training session will have inherent stochasticity. We \n",
    "    will build many models and pick the best one\n",
    "    \"\"\"\n",
    "    def __init__(self, data_loader_train, data_loader_test):\n",
    "        self.data_loader_train = data_loader_train\n",
    "        self.data_loader_test = data_loader_test\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Model training function\n",
    "    \n",
    "        fa_module: FAM class object used for initial feature extraction from initial CLS and Feature tensors\n",
    "        proj_module: Projection class object used to reduce FAM features for contrastive learning loss function\n",
    "        supconloss_module: SupConLoss class object that calculates contrastive loss\n",
    "        classifer: Classifier class object that outputs predictions from FAM features\n",
    "        data_loader: DataLoader class object that splits data into batches for model training\n",
    "        optimizer: optimizer object which helps models with parameters converge to proper feature weights\n",
    "        classifier_loss_fn: loss function which helps the classifer converge to proper feature weights\n",
    "    \n",
    "        Returns:\n",
    "            Tuple of average loss at [0] and accuracy at [1] for each epoch\n",
    "        \"\"\"\n",
    "        # Set all networks to train mode\n",
    "        self.fam.train()\n",
    "        self.proj.train()\n",
    "        self.supcon_loss.train()\n",
    "        self.classifier.train()\n",
    "    \n",
    "        # Declare variables to measure batch performance\n",
    "        correct = 0\n",
    "        total_targets = 0\n",
    "        n_batches = 0\n",
    "        train_loss = 0\n",
    "    \n",
    "        for data in self.data_loader_train:\n",
    "            # Start model training\n",
    "            n_batches += 1\n",
    "            self.optimizer.zero_grad()\n",
    "            embs = data[0].squeeze(1)\n",
    "            targets = data[1]\n",
    "            \"\"\"\n",
    "            Model flow: FAM -> projection head -> supcon loss. Predictions are made from FAM features, while\n",
    "            Supconloss is calculated from the projection head.\n",
    "            \"\"\"\n",
    "            fam_output = self.fam(embs)   \n",
    "            proj_output = self.proj(fam_output)\n",
    "            supcon_loss = self.supcon_loss(proj_output, targets)\n",
    "            classifier_output = self.classifier(fam_output)  \n",
    "            classifier_loss = self.classifier_loss(classifier_output, targets)\n",
    "    \n",
    "            # Use a combined supconloss and classifer loss\n",
    "            loss = supcon_loss + classifier_loss \n",
    "            loss.backward()  \n",
    "            self.optimizer.step()  \n",
    "            train_loss += loss.item()\n",
    "    \n",
    "            # Calculate accuracy for the batch\n",
    "            preds = classifier_output.argmax(1)\n",
    "            correct += np.sum(np.array(preds) == np.array(targets))\n",
    "            total_targets += len(targets)\n",
    "    \n",
    "        # Calculate overall accuracy and loss\n",
    "        accuracy = correct / total_targets\n",
    "        average_loss = train_loss / n_batches\n",
    "    \n",
    "        return average_loss, accuracy\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        fa_module: optimized FAM object used to generate features\n",
    "        classifier: optimized Classifier object used to classify features from fa_module\n",
    "        data_loader: DataLoader class object that splits data into batches for model training\n",
    "    \n",
    "        Returns:\n",
    "            accuracy of evaluation\n",
    "        \"\"\"\n",
    "        # Set networks to eval mode\n",
    "        self.fam.eval()  \n",
    "        self.classifier.eval()\n",
    "    \n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        # Evaluation the FAM and Classifier\n",
    "        with torch.no_grad():  \n",
    "            for data in self.data_loader_test:\n",
    "                embs = data[0].squeeze(1)  \n",
    "                targets = data[1].tolist()\n",
    "                \n",
    "                fam_output = self.fam(embs)\n",
    "                final_output = self.classifier(fam_output)\n",
    "                preds = final_output.argmax(1).tolist()\n",
    "                \n",
    "                total += len(preds) \n",
    "                correct += np.sum(np.array(preds) == np.array(targets))  \n",
    "    \n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def gen_model(self):\n",
    "        \"\"\"\n",
    "        Generates the best model based on test set accuracy\n",
    "        \"\"\"\n",
    "        # Declare constants for networks\n",
    "        HIDDEN_SIZE_1 = 256\n",
    "        HIDDEN_SIZE_2 = 128\n",
    "        DROPOUT_PERCENT = 0.3\n",
    "        NUM_CLASSES = 3\n",
    "        LEARNING_RATE = 0.001\n",
    "        STEP_SIZE = 20\n",
    "        GAMMA = 0.5\n",
    "        STOP_EARLY_NUM = 10\n",
    "        NUM_MODELS = 200\n",
    "\n",
    "        # Keep track of the best test accuracy\n",
    "        best_test_acc = 0\n",
    "        for i in tqdm(range(NUM_MODELS)):\n",
    "            self.fam = FAM(NUM_FEATURES, HIDDEN_SIZE_1, DROPOUT_PERCENT)\n",
    "            self.proj = Projection(HIDDEN_SIZE_1, HIDDEN_SIZE_2)\n",
    "            self.classifier = Classifier(HIDDEN_SIZE_1, NUM_CLASSES, DROPOUT_PERCENT)\n",
    "            self.optimizer = optimizer = torch.optim.Adam(list(self.fam.parameters()) + \n",
    "                                             list(self.proj.parameters()) + \n",
    "                                             list(self.classifier.parameters()), \n",
    "                                            lr=LEARNING_RATE)\n",
    "            self.scheduler = scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "            self.classifier_loss = nn.CrossEntropyLoss()\n",
    "            self.supcon_loss = SupConLoss()\n",
    "            i = 0\n",
    "            best_acc = 0\n",
    "            # Utilize 'stopping early' when 10 successive models have failed to improve\n",
    "            while i < STOP_EARLY_NUM:\n",
    "                loss, acc = self.train()  \n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_fam = self.fam.state_dict()  \n",
    "                    best_proj = self.proj.state_dict() \n",
    "                    best_classifier = self.classifier.state_dict()\n",
    "                    i = 0\n",
    "                else:\n",
    "                    i += 1\n",
    "                scheduler.step()\n",
    "\n",
    "            # Load the best training model for test set evaluation\n",
    "            self.fam.load_state_dict(best_fam)\n",
    "            self.proj.load_state_dict(best_proj)\n",
    "            self.classifier.load_state_dict(best_classifier)\n",
    "            test_accuracy = self.evaluate()\n",
    "            \n",
    "            if test_accuracy > best_test_acc:\n",
    "                # If this model performs better than previous models, update overall best\n",
    "                best_test_acc = test_accuracy\n",
    "                self.overall_best_fam = best_fam\n",
    "                self.overall_best_proj = best_proj\n",
    "                self.overall_best_classifier = best_classifier\n",
    "                print('Train Set Accuracy: ' + str(best_acc*100) + '%')\n",
    "                print('Test Set Accuracy: ' + str(test_accuracy*100) + '%')\n",
    "                \n",
    "    def save_model(self, filepath):\n",
    "        # Save the best models to disk\n",
    "        torch.save({'fam_state_dict': self.overall_best_fam,\n",
    "            'proj_state_dict': self.overall_best_proj,\n",
    "            'classifier_state_dict': self.overall_best_classifier,\n",
    "        }, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25075d5f-8384-4236-9906-0ea0f2f31604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 1/200 [00:08<27:23,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 53.029411764705884%\n",
      "Test Set Accuracy: 55.125%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 2/200 [00:19<33:20, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 52.38235294117647%\n",
      "Test Set Accuracy: 55.875%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                                | 3/200 [00:46<57:51, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 54.470588235294116%\n",
      "Test Set Accuracy: 58.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [47:03<00:00, 14.12s/it]\n"
     ]
    }
   ],
   "source": [
    "temp_0_generator = ModelGenerator(data_loader_0_train, data_loader_0_test)\n",
    "temp_0_generator.gen_model()\n",
    "temp_0_generator.save_model('temp_0_models.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc85f8f0-c25a-4c1f-83fb-58fcc407fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 1/200 [00:16<53:28, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 52.485714285714295%\n",
      "Test Set Accuracy: 50.875%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                | 5/200 [00:54<33:49, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 52.142857142857146%\n",
      "Test Set Accuracy: 53.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▊                                                                            | 12/200 [02:46<49:29, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 52.51428571428571%\n",
      "Test Set Accuracy: 54.50000000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████████████▊                                     | 107/200 [27:03<30:40, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 52.0%\n",
      "Test Set Accuracy: 54.625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [51:27<00:00, 15.44s/it]\n"
     ]
    }
   ],
   "source": [
    "temp_7_generator = ModelGenerator(data_loader_7_train, data_loader_7_test)\n",
    "temp_7_generator.gen_model()\n",
    "temp_7_generator.save_model('temp_7_models.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a11cbdfd-cfe3-4d2a-a3fe-09bcc4ddd1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                               | 1/200 [00:21<1:12:23, 21.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 52.142857142857146%\n",
      "Test Set Accuracy: 51.87500000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 2/200 [00:35<55:53, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 51.42857142857142%\n",
      "Test Set Accuracy: 52.37500000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                | 5/200 [01:28<55:19, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 51.74285714285715%\n",
      "Test Set Accuracy: 52.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▊                                                                               | 7/200 [02:00<52:50, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 51.97142857142857%\n",
      "Test Set Accuracy: 53.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████▍                 | 156/200 [34:40<10:08, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 51.51428571428571%\n",
      "Test Set Accuracy: 53.87499999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [42:42<00:00, 12.81s/it]\n"
     ]
    }
   ],
   "source": [
    "temp_14_generator = ModelGenerator(data_loader_14_train, data_loader_14_test)\n",
    "temp_14_generator.gen_model()\n",
    "temp_14_generator.save_model('temp_14_models.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "faad0885-d75d-4479-acdc-e190b15b6399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                               | 1/200 [00:19<1:04:59, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 49.27619047619047%\n",
      "Test Set Accuracy: 52.07692307692307%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                               | 2/200 [00:44<1:14:14, 22.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 50.114285714285714%\n",
      "Test Set Accuracy: 52.730769230769226%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                              | 3/200 [01:11<1:21:54, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 49.78095238095238%\n",
      "Test Set Accuracy: 54.19230769230769%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                              | 5/200 [02:20<1:39:29, 30.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 50.05714285714286%\n",
      "Test Set Accuracy: 54.230769230769226%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████                                                                  | 33/200 [15:42<1:29:46, 32.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 50.08571428571429%\n",
      "Test Set Accuracy: 54.34615384615385%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████▎                         | 134/200 [1:04:06<33:58, 30.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: 50.32380952380953%\n",
      "Test Set Accuracy: 54.46153846153846%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [1:33:57<00:00, 28.19s/it]\n"
     ]
    }
   ],
   "source": [
    "temp_all_generator = ModelGenerator(data_loader_all_train, data_loader_all_test)\n",
    "temp_all_generator.gen_model()\n",
    "temp_all_generator.save_model('temp_all_models.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb7ace-5db5-4280-b259-4b1befae40b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
